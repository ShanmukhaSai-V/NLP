{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVgzXuExNW1j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBlYs_PnNW1l"
      },
      "outputs": [],
      "source": [
        "# Read the text file\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "corpus = text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqYQpochNW1m"
      },
      "outputs": [],
      "source": [
        "# Data Generator class\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, texts, vectorizer, seq_length=5, batch_size=32, shuffle=True):\n",
        "        self.texts = texts\n",
        "        self.vectorizer = vectorizer\n",
        "        self.seq_length = seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.texts) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_texts = self.texts[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X, y = self.__data_generation(batch_texts)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.texts))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, batch_texts):\n",
        "        X, y = [], []\n",
        "        for text in batch_texts:\n",
        "            encoded = self.vectorizer(text)\n",
        "            for i in range(1, len(encoded)):\n",
        "                X.append(encoded[:i])\n",
        "                y.append(encoded[i])\n",
        "\n",
        "        X = pad_sequences(X, maxlen=self.seq_length, padding='pre')\n",
        "        y = np.array(y)\n",
        "        return np.array(X), y\n",
        "\n",
        "# Function to encode text using TensorFlow TextVectorization\n",
        "def encode_text_tf(text, vectorizer):\n",
        "    return vectorizer([text]).numpy()[0]\n",
        "\n",
        "# Function to decode text using TensorFlow TextVectorization\n",
        "def decode_text_tf(indices, index_word):\n",
        "    return ' '.join([index_word[idx] for idx in indices if idx != 0])\n",
        "\n",
        "# Function to encode text using CountVectorizer\n",
        "def encode_text_cv(text, vectorizer):\n",
        "    return vectorizer.transform([text]).toarray()[0]\n",
        "\n",
        "# Function to decode text using CountVectorizer\n",
        "def decode_text_cv(indices, index_word):\n",
        "    return ' '.join([index_word[idx] for idx in indices if idx in index_word])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPM9awEsNW1m"
      },
      "outputs": [],
      "source": [
        "# Build CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vectorizer.fit(corpus)\n",
        "vocab_size_cv = len(count_vectorizer.vocabulary_)\n",
        "word_index_cv = {v: k for k, v in count_vectorizer.vocabulary_.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-JaEflkNW1n"
      },
      "outputs": [],
      "source": [
        "# Build TensorFlow TextVectorization\n",
        "seq_length = 5\n",
        "vectorizer_tf = TextVectorization(max_tokens=20000, output_sequence_length=seq_length)\n",
        "vectorizer_tf.adapt(corpus)\n",
        "vocab_size_tf = len(vectorizer_tf.get_vocabulary())\n",
        "index_word_tf = {idx: word for idx, word in enumerate(vectorizer_tf.get_vocabulary())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G0wYcVRNW1n"
      },
      "outputs": [],
      "source": [
        "# Create data generators for both vectorizers\n",
        "train_gen_cv = DataGenerator(corpus, lambda text: count_vectorizer.transform([text]).toarray()[0], seq_length=seq_length)\n",
        "train_gen_tf = DataGenerator(corpus, vectorizer_tf)\n",
        "\n",
        "# Define model\n",
        "def build_lstm_model(vocab_size, embedding_dim=128, seq_length=5):\n",
        "    inputs = Input(shape=(seq_length,))\n",
        "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length)(inputs)\n",
        "    x = LSTM(128)(x)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the models\n",
        "model_cv = build_lstm_model(vocab_size_cv)\n",
        "model_cv.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_tf = build_lstm_model(vocab_size_tf)\n",
        "model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJg_x6jwNW1n"
      },
      "outputs": [],
      "source": [
        "# Train using the CountVectorizer data generator\n",
        "model_cv.fit(train_gen_cv, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbH5ZET6NW1o"
      },
      "outputs": [],
      "source": [
        "# Train using the TensorFlow TextVectorization data generator\n",
        "model_tf.fit(train_gen_tf, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsBlAfKeNW1o"
      },
      "outputs": [],
      "source": [
        "# Predict next word function\n",
        "def predict_next_word(model, input_text, vectorizer, decode_fn, seq_length):\n",
        "    input_seq = pad_sequences([vectorizer(input_text)], maxlen=seq_length, padding='pre')\n",
        "    prediction = model.predict(input_seq)\n",
        "    predicted_word_idx = np.argmax(prediction, axis=-1)[0]\n",
        "    return decode_fn([predicted_word_idx], index_word_cv if vectorizer == count_vectorizer else index_word_tf)\n",
        "\n",
        "# Example usage for next word prediction\n",
        "test_text = \"the quick brown\"\n",
        "predicted_word_cv = predict_next_word(model_cv, test_text, encode_text_cv, decode_text_cv, seq_length)\n",
        "predicted_word_tf = predict_next_word(model_tf, test_text, encode_text_tf, decode_text_tf, seq_length)\n",
        "\n",
        "print(f\"Predicted next word (CountVectorizer): {predicted_word_cv}\")\n",
        "print(f\"Predicted next word (TensorFlow TextVectorizer): {predicted_word_tf}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Version10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}